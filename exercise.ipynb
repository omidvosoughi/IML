{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrance Test\n",
    "\n",
    "In the following tasks, you are guided to solve a multiclassification task to determine the `type` of wheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('lightning')\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "pl.utilities.distributed.log.setLevel(logging.ERROR)\n",
    "pl.utilities.seed.log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparing Dataset\n",
    "\n",
    "Before a model can be trained, the dataset has to be loaded, preprocessed and splitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "- Read in the csv file `wheat_seeds.csv`, which consists of 199 data instances with 8 features each. \n",
    "- Divide the data into X and y:\n",
    "  - X should be a two-dimensional numpy array with the shape (199,7) derived from all columns except the last.\n",
    "  - y should be a two-dimensional numpy array with the shape (199,) derived from the last column. This last column contains the `type` value, which we want to determine for unseen\n",
    "    data in the end.\n",
    "- Shuffle the data using `sklearn.utils.shuffle`. Set `random_state=0`.\n",
    "\n",
    "Hints:\n",
    "- Use pandas to read in the file easily and convert them to a numpy array.\n",
    "- Use `shape` to check the dimension of a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = None, None\n",
    "X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing\n",
    "\n",
    "- Take care of NaN/None values in X and y. Replace them simply with zeros here.\n",
    "- Normalization is most of the time necessary to ensure good convergence in a Neural Network.\n",
    "Transform X s.t. the values are between 0 and 1.\n",
    "\n",
    "Hints:\n",
    "- NaN/None values have to be replaced first.\n",
    "- Use [sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) for the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Splitting\n",
    "\n",
    "- Split X and y into training and validation data.\n",
    "- Use the ratio 6:4 for training and validation data, respectively.\n",
    "\n",
    "Hint: You can use vanilla numpy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = None, None\n",
    "X_val, y_val = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Multi Layer Perceptron\n",
    "\n",
    "We now want to use our training and validation data to fit a model. As model we use a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Defining Sequential Model\n",
    "\n",
    "- To implement the model, we use pytorch lightning. Please have a look into the [introductions](https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html) to get a short overview.\n",
    "- Define a [sequential model](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) in the following way:\n",
    "    - The first linear layer should take the number of features from X (already given as `input_units`) as `in_features` and `num_hidden_layers` as `out_features`.\n",
    "    - The last linear layer should have `output_units` as `out_features`.\n",
    "    - In between, use `num_hidden_layers` and `num_hidden_units` to set the hidden linear layers and\n",
    "      their features, respectively.\n",
    "    - `num_hidden_layers` tells us how many layers should be used and\n",
    "      `num_hidden_units` how many units in each of these layers are used.\n",
    "    - After every linear layer, a ReLU as activation function follows.\n",
    "    - However, please leave out the activation after the last linear layer.\n",
    "\n",
    "Hint: Use *list to use a list as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequential_model(num_hidden_layers,\n",
    "                         num_hidden_units,\n",
    "                         input_units,\n",
    "                         output_units):\n",
    "    \"\"\"\n",
    "    Returns a sequential model with 2+num_hidden_layers linear layers.\n",
    "    All linear layers (except the last one) are followed by a ReLU function.\n",
    "\n",
    "    Parameters:\n",
    "        num_hidden_layers (int): The number of hidden layers.\n",
    "        num_hidden_units (int): The number of features from the hidden\n",
    "            linear layers.\n",
    "        input_units (int): The number of input units.\n",
    "            Should be number of features.\n",
    "        output_units (int): The number of output units. In case of regression task,\n",
    "            it should be one.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Sequential): Neural network as sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Since we have numpy data, it is required to convert\n",
    "    them into PyTorch tensors first.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        \n",
    "        self.y = torch.zeros((self.X.shape[0], 1), dtype=torch.float32)\n",
    "        if y is not None:\n",
    "            y = y.astype(np.float32)\n",
    "            self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Multi Layer Perceptron wrapper for pytorch lightning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequential_model, verbose=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sequential_model: Underlying Neural Network.\n",
    "            verbose (bool): If true, the highest val accuracy is plotted after every epoch.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        self.highest_val_accuracy = 0\n",
    "\n",
    "        # Important to init the weights the same way\n",
    "        pl.seed_everything(0)\n",
    "        self.model = sequential_model\n",
    "        \n",
    "        # (Multi-)Classification problem\n",
    "        self.loss_fn = nn.CrossEntropyLoss() \n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y_test):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy for `y_pred` and `y_test`.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim=1)    \n",
    "        \n",
    "        correct_pred = (y_pred_tags == y_test).float()\n",
    "        acc = correct_pred.sum() / len(correct_pred)\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        \"\"\"\n",
    "        Receives the validation data and calculates the accuracy on them.\n",
    "\n",
    "        Parameters:\n",
    "            batch: Tuple of validation data.\n",
    "\n",
    "        Returns: \n",
    "            metrics (dict): Dict with val accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "        \n",
    "        val_accuracy = self.calculate_accuracy(y_hat, y)\n",
    "        return {'val_accuracy': val_accuracy}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Collects the outputs from `validation_step` and sets\n",
    "        the average for the accuracy.\n",
    "\n",
    "        Parameters:\n",
    "            outputs: List of dicts from `validation_step`.\n",
    "        \"\"\"\n",
    "\n",
    "        val_accuracy = torch.stack([o['val_accuracy'] for o in outputs]).numpy().flatten()\n",
    "        \n",
    "        val_accuracy = float(np.mean(val_accuracy))\n",
    "        if val_accuracy > self.highest_val_accuracy:\n",
    "            self.highest_val_accuracy = val_accuracy\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"{self.current_epoch}: {self.highest_val_accuracy}\")\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        \"\"\"\n",
    "        Receives the training data and calculates\n",
    "        cross entropy as loss, which is used to train\n",
    "        the classifier.\n",
    "\n",
    "        Parameters:\n",
    "            batch: Tuple of training data.\n",
    "\n",
    "        Returns: \n",
    "            loss (Tensor): Loss of current step. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "\n",
    "        return self.loss_fn(y_hat, y.long())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures Adam as optimizer.\n",
    "\n",
    "        Returns:\n",
    "            optimizer (torch.optim): Optimizer used internally by\n",
    "                pytorch lightning.\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def fit(self, X_train, y_train,\n",
    "                  X_val, y_val,\n",
    "                  learning_rate=1e-1,\n",
    "                  num_epochs=10,\n",
    "                  batch_size=8):\n",
    "        \"\"\"\n",
    "        Fits the model with training data. Model is validated after every epoch on the validation\n",
    "        data.\n",
    "        \n",
    "        Parameters:\n",
    "            X_train, y_train: Training data.\n",
    "            X_val, y_val: Validation data.\n",
    "            learning_rate (float): Learning rate used in the optimizer.\n",
    "            num_epochs (int): Number of epochs.\n",
    "            batch_size (int): How many instances are used to update the weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        pl.seed_everything(0)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.trainer = pl.Trainer(\n",
    "            num_sanity_val_steps=0,  # No validation sanity\n",
    "            max_epochs=num_epochs,  # We only train one epoch\n",
    "            progress_bar_refresh_rate=0,  # No progress bar\n",
    "            weights_summary=None  # No model summary\n",
    "        )\n",
    "\n",
    "        # Define training loader\n",
    "        # `train_loader` is a lambda function, which takes batch_size as input\n",
    "        train_loader = DataLoader(\n",
    "            PyTorchDataset(X_train, y_train),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "\n",
    "        # Define validation loader\n",
    "        val_loader = DataLoader(\n",
    "            PyTorchDataset(X_val, y_val),\n",
    "            batch_size=1,\n",
    "            num_workers=0)\n",
    "\n",
    "        # Train model\n",
    "        self.trainer.fit(self, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training the model\n",
    "\n",
    "Define the input and the output units and pass them to the `get_sequential_model` function.\\\n",
    "The output units should be determined with the help of `np.unique` since we have one neuron per class. The input\n",
    "units are the number of used X features. Choose the other parameters as you please.\n",
    "\n",
    "Afterwards, initialize the `MLP` model with verbose and call its fit method to verify that\n",
    "everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_units = None\n",
    "output_units = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Random Search\n",
    "\n",
    "In practice, it is often not clear how to choose hyperparameters for a specific problem. Random\n",
    "search can be used to found a suitable configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Writing Random Search\n",
    "\n",
    "- Perform random search to find the best configuration for the given multiclassification problem.\n",
    "- Write a loop that trains a MLP based on the config.\n",
    "- All results should be saved in the dict `results` with config as key and accuracy as value.\n",
    "\n",
    "Hints:\n",
    "- Use model.highest_val_accuracy to judge the performance of a configuration. \n",
    "- Access hyperparameters from config with e.g. `config[\"lr\"]`.\n",
    "- Use all defined hyperparameters.\n",
    "- Execution may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "from tqdm import tqdm\n",
    "\n",
    "cs = CS.ConfigurationSpace(seed=0)\n",
    "\n",
    "learning_rate_hp = CSH.UniformFloatHyperparameter('learning_rate', lower=1e-5, upper=1e-1)\n",
    "num_hidden_layers = CSH.UniformIntegerHyperparameter('num_hidden_layers', lower=0, upper=4)\n",
    "num_hidden_units = CSH.UniformIntegerHyperparameter('num_hidden_units', lower=1, upper=int(X_train.shape[1] * 2))\n",
    "batch_size_hp = CSH.UniformIntegerHyperparameter('batch_size', lower=8, upper=int(len(X_train)))\n",
    "cs.add_hyperparameters([learning_rate_hp, num_hidden_layers, num_hidden_units, batch_size_hp])\n",
    "\n",
    "results = {}\n",
    "for config in tqdm(cs.sample_configuration(25)):\n",
    "    # TODO: Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Plotting\n",
    "\n",
    "- Print the best and the worst configuration with their corresponding performance.\n",
    "- Use matplotlib to plot a step function (use `x` and `y`), showing the highest accuracy found so far over time (in our case: number of evaluated configurations).\n",
    "- Make sure the steps are placed \"post\".\n",
    "\n",
    "Hints:\n",
    "- Use the results dict from the exercise before.\n",
    "- Have a look into the [API](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.step.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "highest_accuracy = None\n",
    "lowest_accuracy = None\n",
    "\n",
    "best_config = None\n",
    "worst_config = None\n",
    "\n",
    "print(f\"Found best {best_config} with {highest_accuracy*100}%.\")\n",
    "print(f\"Found worst {worst_config} with {lowest_accuracy*100}%.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0d0a4d5b6a953c55672bafe1463fc39acea2a8d795c189bf7d3a4f6394ebb6c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('PreExam': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
